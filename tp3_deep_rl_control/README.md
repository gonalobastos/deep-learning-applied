# TP3 â€“ Deep Reinforcement Learning for Agent Control ğŸš—ğŸ¤–

This project explores the use of **Deep Reinforcement Learning (DeepRL)** to control agents in simulated environments, focusing on state representations based on images and pose vectors.

## ğŸ§© Part I â€” OpenAI CarRacing-v2

A custom CNN was implemented and integrated with:

- DQN  
- Double DQN  
- Dueling DQN  
- Double Dueling DQN  

### ğŸ® Control modes:
- **Continuous control** (3 variables: steering, gas, brake)  
- **Discrete control** (5 actions: nothing, left, right, gas, brake)  

### ğŸ”§ CNN Architecture
- 3 convolutional layers (32â†’64 filters) + GELU + MaxPooling  
- Final fully connected layers  
- Dueling variant: separate branches for `Value` and `Advantage`

### ğŸ“Š Performance Summary (example)

| Model           | Mode      | Avg. Normal Tracks | Avg. Random Tracks |
|------------------|-----------|---------------------|---------------------|
| DQN              | Continuous | ~840                | ~-57                |
| Dueling DQN      | Discrete   | **~810**            | ~-42                |

> The best overall performance was achieved with **Dueling DQN using discrete control**.

---

## ğŸ¤– Part II â€” Custom Environment (Mobile Robot)

The best-performing model from Part I (Dueling DQN â€“ discrete mode) was applied to a custom environment to control a mobile robot in two scenarios:

1. **PoseOnly** â€“ input: vector indicating the direction to the goal  
2. **PoseAndLocalMap** â€“ input: pose + flattened local map with obstacle information  

### ğŸ”§ MLP Architecture
- 3 linear layers with GELU activations  
- Separate branches for Value and Advantage streams  
- Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))

### ğŸ“ˆ Observations

| Scenario            | Episodes to Stabilize | Avg. Reward |
|---------------------|-----------------------|-------------|
| PoseOnly            | ~300                  | still negative |
| PoseAndLocalMap     | ~1000â€“1500            | ~-200         |

> The agent learned to avoid obstacles and reach the goal, though longer training and better regularization may improve consistency.

---

## ğŸ“ Directory Contents

- `APA_Assign3_Goncalo_Bastos_Leonardo_Cordeiro.pdf` â€” technical report  
- `APA2024_DeepRL_Part1.ipynb` â€” full implementation of part1
-  `APA2024_DeepRLPart2.ipynb` â€” full implementation of part2
- `results/` â€” training plots, visual results and GIFS or videos

---

## ğŸ‘¨â€ğŸ’» Authors

- GonÃ§alo Bastos â€“ eusoudebastos@gmail.com  
- Leonardo Cordeiro â€“ leoleocordeiro@gmail.com  
University of Coimbra â€“ Electrical and Computer Engineering

